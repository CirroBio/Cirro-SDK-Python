{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Uploading a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pubweb import DataPortal\n",
    "portal = DataPortal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To upload a dataset to the portal, you need to specify:\n",
    "\n",
    "  1. The project which it will be uploaded to\n",
    "  2. The process which defines the type of data being uploaded\n",
    "  3. The name and description of the dataset\n",
    "  4. The path to the folder which contains the files to upload\n",
    "  5. Optionally, a filtered list of files which should be uploaded (default is to upload all files in a folder)\n",
    "\n",
    "First, get the project where the dataset will be uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Test Project\n",
      "ID: 9a31492a-e679-43ce-9f06-d84213c8f7f7\n"
     ]
    }
   ],
   "source": [
    "project = portal.get_project_by_name(\"Test Project\")\n",
    "print(f\"Name: {project.name}\")\n",
    "print(f\"ID: {project.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, select the process which will be used for ingesting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 different ingest processes available\n",
      "\n",
      "Selected process:\n",
      "Name: Paired DNAseq (FASTQ)\n",
      "Id: paired_dnaseq\n",
      "Description: FASTQ files generated from paired-end sequencing of DNA libraries\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all available ingest processes\n",
    "ingest_processes = portal.list_processes(ingest=True)\n",
    "print(f\"There are {len(ingest_processes):,} different ingest processes available\")\n",
    "# Uncomment the line below to print a list of all available ingest processes\n",
    "# print(ingest_processes.description())\n",
    "\n",
    "# Select the process used to ingest paired DNA sequencing data\n",
    "process = ingest_processes.get_by_name(\"Paired DNAseq (FASTQ)\")\n",
    "print(\"\\nSelected process:\")\n",
    "print(str(process))\n",
    "\n",
    "# To do the above in a single step, simply run:\n",
    "# process = portal.get_process_by_name(\"Paired DNAseq (FASTQ)\", ingest=True)\n",
    "\n",
    "# When uploading the dataset (below), the process used for ingest can be specified\n",
    "# either with the process object which was obtained above, or simply with the name\n",
    "# of that process (or its ID)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If you want to upload just a subset of files from a folder, you just need to make a list of the filenames (paths relative to the upload directory) which you want to upload. This overrides the default behavior of uploading everything from the upload directory.\n",
    "\n",
    "We've included two helper functions to get a list of files in the specified directory and filter them.\n",
    "\n",
    "You can also manually create the list of files (using those relative paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test.R1.fastq.gz', 'test.R2.fastq.gz']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pubweb.file_utils import get_files_in_directory, filter_files_by_pattern\n",
    "\n",
    "directory_to_upload = '/tmp'\n",
    "\n",
    "files = get_files_in_directory(directory_to_upload)\n",
    "files_to_upload = filter_files_by_pattern(files, '*.fastq.gz')\n",
    "files_to_upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in information on your new dataset in the `name` and `description` variables and then run to check the files and upload new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading file test.R1.fastq.gz (180.76 KB) | 100.0%|█████████████████████████ | 669kB/s\n",
      "Uploading file test.R2.fastq.gz (180.76 KB) | 100.0%|█████████████████████████ | 1.30MB/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Test dataset\n",
      "Id: beccbeea-5ad4-4a76-9c63-08d5d9b63f7b\n",
      "Description: \n",
      "Status: COMPLETED\n"
     ]
    }
   ],
   "source": [
    "# Upload the data and return the new dataset object\n",
    "uploaded_dataset = project.upload_dataset(\n",
    "    name = 'Test dataset',\n",
    "    description = '',\n",
    "    upload_folder = '/tmp',\n",
    "    files = files_to_upload,\n",
    "    process = \"Paired DNAseq (FASTQ)\"\n",
    ")\n",
    "\n",
    "print(str(uploaded_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was an example of what a successful upload looks like.\n",
    "\n",
    "Next, let's take a look at one of the most common issues that you might run into. Namely, if you try to upload files which don't match the pattern expected, it will raise an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File pattern expected for 10X data (note the sample index number and lane ID):\n",
      "[\n",
      "   {\n",
      "      \"glob\": \"*_S*_L???_{I,R}{1,2}_001.fastq.gz\",\n",
      "      \"min\": null,\n",
      "      \"max\": null,\n",
      "      \"description\": \"Paired FASTQ (Illumina Format)\",\n",
      "      \"isSample\": null,\n",
      "      \"sampleMatchingPattern\": \"(?P<sampleName>\\\\S*)_S(?P<sampleIndex>\\\\S*)_L(?P<lane>\\\\S*)_(?P<read_index>I|R)(?P<read>1|2)_001\\\\.fastq\\\\.gz\"\n",
      "   }\n",
      "]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Files do not match dataset type. Expected file type requirements: \nPaired FASTQ (Illumina Format) *_S*_L???_{I,R}{1,2}_001.fastq.gz",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ck/j40906kx3mj90bcc8qs7gyxm0000gp/T/ipykernel_83747/2225702019.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Try to upload the data (which will cause an error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m project.upload_dataset(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Test dataset'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/PubWeb-client/pubweb/sdk/project.py\u001b[0m in \u001b[0;36mupload_dataset\u001b[0;34m(self, name, description, process, upload_folder, files)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# Make sure that the files match the expected pattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mcheck_dataset_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_mapping_rules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupload_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# Create the ingest process request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/PubWeb-client/pubweb/file_utils.py\u001b[0m in \u001b[0;36mcheck_dataset_files\u001b[0;34m(files, file_mapping_rules, directory)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_mapping_rules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         raise ValueError(\"Files do not match dataset type. Expected file type requirements: \\n\" + \"\\n\".join(\n\u001b[0m\u001b[1;32m    151\u001b[0m             [f\"{rule.get('description', '')} {rule.get('glob')}\" for rule in file_mapping_rules]))\n",
      "\u001b[0;31mValueError\u001b[0m: Files do not match dataset type. Expected file type requirements: \nPaired FASTQ (Illumina Format) *_S*_L???_{I,R}{1,2}_001.fastq.gz"
     ]
    }
   ],
   "source": [
    "import json\n",
    "ingest_10X = portal.get_process_by_name(\"Single-cell sequencing data (10X)\", ingest=True)\n",
    "print(\"File pattern expected for 10X data (note the sample index number and lane ID):\")\n",
    "print(json.dumps(ingest_10X.file_mapping_rules, indent=3))\n",
    "\n",
    "# Try to upload the data (which will cause an error)\n",
    "project.upload_dataset(\n",
    "    name = 'Test dataset',\n",
    "    description = '',\n",
    "    upload_folder = '/tmp',\n",
    "    files = files_to_upload,\n",
    "    process = \"Single-cell sequencing data (10X)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
